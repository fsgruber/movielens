---
title: 'HarvardX: PH125.9x'
author: "Franz Gruber"
date: "06/06/2020"
output:
  pdf_document:
    citation_package: natbib
subtitle: "| \n|  _Data Science: Capstone _\n|  \n|  __MovieLens recommendation system__\n"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
```

## Introduction

Many companies use __recommendation systems__ (or __recommender systems__^[https://en.wikipedia.org/wiki/Recommender_system]) to predict what _items_ (e.g. movies, groceries, music, ...) a _user_ might prefer, based on previous user ratings. In 2006, the streaming platform Netflix offered $1,000,000 in a challenge to enhance their recommendation algorithm (called _Cinematch_) by 10% or more^[more information on: https://en.wikipedia.org/wiki/Netflix_Prize]. Here, we want to generate a competetive recommendation system (a machine learning model) to predict a user's movie rating. For this task a smaller subset of the MovieLens dataset generated by the GroupLens research lab^[Department of Computer Science and Engineering at the University of Minnesota, Twin City; https://grouplens.org/about/what-is-grouplens] will be used. This subset includes 10 million ratings, of 10,000 Movies by 72,000 users (release date 01/2009^[https://grouplens.org/datasets/movielens/10m]). We will use the root mean squared error (__RMSE__), as read-out to compare and improve our models. We start building a simple regression model, add several effects (or _biases_), apply regularization to add a penaltiy to our _biases_ and finally try out a more sophisticated approach using matrix factorization.



## Methods

The full code is provided in the _movielens.R_ file. Only show essential code snippets will be shown here.

### Dataset generation
This section is addapted from _HarvardX: PH125.9x_
```{r load-data}
#if the following packages are not installed, please use install.packages()
if(!require(tidyverse)) install.packages("tidyverse")
if(!require(caret)) install.packages("caret")
if(!require(data.table)) install.packages("data.table")
if(!require(recosystem)) install.packages("recosystem")
if(!require(kableExtra)) install.packages("kableExtra")
if(!require(gridExtra)) install.packages("gridExtra")

#plotting theme
theme_set(theme_bw())
#personal preference
theme_update(
  panel.grid.major = element_blank(),
  panel.grid.minor = element_blank()
)



#assign a temp file for data download
dl <- tempfile()
download.file('http://files.grouplens.org/datasets/movielens/ml-10m.zip', dl)

#load ratings table using fred; substitute "::" with tab
ratings <- data.table::fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))


#load movies tables
movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)

colnames(movies) <- c("movieId", "title", "genres")
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
                                           title = as.character(title),
                                           genres = as.character(genres))


#combine ratings and movie table
movielens <- left_join(ratings, movies, by = "movieId")


set.seed(1, sample.kind="Rounding")

# Validation set will be 10% of MovieLens data
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]


#remove users and movies from test set (validation), which are not part in the train set (edx set)
validation <- temp %>%
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")


# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

#remove unused objects
rm(dl, ratings, movies, test_index, temp, movielens, removed)


#transform timestamp column into human readable format and generate week column
edx <- edx %>% 
  mutate(timestamp = lubridate::as_datetime(timestamp),
         week = lubridate::round_date(timestamp, unit = 'week'))

validation <- validation %>% 
  mutate(timestamp = lubridate::as_datetime(timestamp),
         week = lubridate::round_date(timestamp, unit = 'week'))


```

The _MovieLens 10M_ dataset was downloaded from https://grouplens.org/datasets/movielens/10m. A `ratings` table containing _userId, movieId, rating, timestamp_ columns can be merged with a `movies` table containing _movieId, titles, genres_ on _movieId_ columns. The resulting `data.frame` is partitioned into `edx` (90 % of the data; used for model training) and `validation` (10 % of data; used for final model evaluation) using the `createDataPartition` function of the `caret` package.


### Data cleaning

Both, `edx` as well as the `validation` contain a _timestamp_ column, which contains _UNIX_ time format. This can be transformed into human readable format using the `as_datetime` function of the `lubridate` package. In addition we will generate a new column _week_ using the `round_date` function of the `lubridate` package, setting the parameter `unit = 'week'`.



```{r final_table}

head(edx) %>% 
  knitr::kable(., caption = '`edx` after data cleaning', format = "latex") %>% 
  kable_styling(latex_options = c('striped', 'scale_down', "HOLD_position"))

```


### Data Exploration

Our `edx` dataset consists of `r dim(edx)[1]` _rows_ (or user ratings) and `r dim(edx)[2]`  _columns_. There are no missing values in any of the columns:

```{r check_na, echo = TRUE}

is.na(edx) %>% any()


```
The number of users and movies are summarized below. We can see that if we would multiply $users \times movies$, we would get a much higher number of user ratings, than we acutally have. This shows that now every user rated every movie.


```{r user_movie_summary}

edx %>%
summarize(movies = n_distinct(movieId),
            users = n_distinct(userId),
          movie_times_users = movies*users) %>% 
  knitr::kable(., caption = 'User/Movie Summary') %>% 
  kable_styling(latex_options = c("HOLD_position"))

```

We can visualize this by selecting 50 random users and any _movieId_ below 100:

```{r movie_user_matrix, fig.align='center', fig.width=7, fig.height=3}

set.seed(1984, sample.kind = "Rounding")
userid <- sample(edx$userId, 50)

edx %>% 
  filter(userId %in% userid) %>%
  filter(movieId <=100) %>% 
  select(userId, movieId, rating) %>%
  #recode userId + movieId for plotting
  group_nest(userId) %>%
  mutate(user_id = seq(1, length(userId))) %>%
  unnest() %>%
  group_nest(movieId) %>%
  mutate(movie_id = seq(1, length(movieId))) %>%
  unnest() %>%
  ggplot(aes(movie_id, user_id, fill = as.character(rating))) +
  geom_tile() +
  scale_fill_brewer(name = 'Rating', palette = "Set3") +
  labs(title = 'Movie-User-Rating matrix',
       x = 'Movie ID', y = 'User ID') +
  theme(
    axis.ticks.x = element_blank(),
    axis.ticks.y = element_blank(),
    axis.text.x = element_blank(),
    axis.text.y = element_blank()
  )

```

Data in the `edx` set contains user ratings from `r edx %>% pull(timestamp) %>% min()` to `r edx %>% pull(timestamp) %>% max()`.

The number of movie ratings is visualized as a histogram below. We can see that some movies get very few ratings (some only 1 rating), while other movies get > 10,000 ratings.

```{r movie-ratings-plot, fig.align='center', fig.width=3, fig.height=2.5}

edx %>% 
  count(movieId) %>% 
  ggplot(aes(n)) +
  geom_histogram(bins = 30, color = 'black') +
  scale_x_log10() +
  labs(y = 'Number of Movies', 
       title = "Count of movie ratings", 
       x = 'Number of ratings')

```
The `median` and `mean` number of movie ratings are listed below. The median is more robust against movies with extremely high number of ratings.

```{r movie-ratings-median-mean, echo = TRUE}


edx %>% 
  count(movieId) %>% 
  summarize(mean = mean(n),
            median = median(n))

```

We can also summarize the number of user ratings per user as a histogram and see that some users rate more movies than others:

```{r user-ratings-plot, fig.align='center', fig.width=3, fig.height=2.5}

edx %>% 
  count(userId) %>% 
  ggplot(aes(n)) +
  geom_histogram(bins = 30, color = 'black') +
  scale_x_log10() +
  labs(title = 'User ratings',
       x = 'Number of ratings',
       y = 'Number of Users')

```

The `median` and `mean` number of user ratings are listed below.

```{r mean-median-user-ratings, echo = TRUE}

edx %>% 
  count(userId) %>% 
  summarize(mean = mean(n),
            median = median(n))

```

Looking at the top 10 movies with the highest number of ratings, we can see _Pulp Fiction_ with the most user ratings, amongst other popular movies.

```{r top-ten-user-rating}

edx %>% 
  group_by(movieId, title) %>% 
  summarize(n = n()) %>% 
  ungroup() %>% 
  top_n(10, n) %>%
  arrange(desc(n)) %>% 
  knitr::kable(., caption = 'Top 10 Movies with highest number of ratings') %>% 
  kable_styling(latex_options = c("HOLD_position"))

```

Next we want to see, how the weekly average user rating changed over time between 1995-2009. We can see that there are some variation in weekly average user ratings over time.

```{r average_user_rating_vs_time, fig.align='center', fig.width=4, fig.height=3.5}

edx %>% 
  group_by(week) %>% 
  summarize(avg_rating = mean(rating)) %>%
  ggplot(aes(week, avg_rating)) +
  geom_point(alpha = 0.5) +
  geom_smooth() +
  labs(title = 'Average rating per week',
       subtitle = 'each dot = 1 week',
       x = 'Time', y = 'AVG rating')

```

Finally, we will have a look at movie genres. We will separate the _genres_ column using the `separate_rows` function of the `tidyr` package. As with the number of movie ratings and user ratings, we can see that some genres are more popular than others:


```{r genres_barplot, fig.align='center', fig.width=8, fig.height=4}

#split genres; it is faster to split within a smaller table and then join by movieId
genres_split <- edx %>% 
  distinct(movieId, .keep_all = T) %>% 
  separate_rows(genres, sep = '\\|') %>% 
  select(movieId, genres)


#plot genres count
edx %>% 
  select(-genres) %>% 
  inner_join(genres_split) %>% 
  count(genres) %>% 
  # top_n(10, n) %>% 
  ggplot(aes(x = n, y = reorder(genres, n))) +
  geom_bar(stat = 'identity') +
  labs(title = 'Movie genres',
       x = 'Number of ratings',
       y = '') +
  scale_x_continuous(labels = scales::comma)


```


### Insights gained

This section contained typical tasks of a data science project: gathering, cleaning and exploration of data. The `Movielens` dataset is available online, so there was no need to perform any web scraping or pdf parsing. The data set comes in tidy format^[https://www.jstatsoft.org/article/view/v059i10]. For our purpose, we only had to transform the _UNIX_ time into a human readable format using the `lubridate` package, and split the _genres_ column into single genres. 

After data cleaning, we explored the data using plots and summary tables. We saw that some movies get more ratings than others; some users rate more than others; there seems to be fluctuations in weekly average movie ratings over time; some genres receive more ratings than others. We can use some of this information gained for our main goal, which is to generate a competitive movie recommendation system.


### Loss function

Our aim is to create a movie recommendation system. We will use a modeling approach and for this we require a loss function, which informs us about our prediction error. Furthermore, we use the information of our loss function to improve our model. 

Here, we will use the residual mean squared error (RMSE), which is a popular choice in the field of machine learning. The RMSE is defined as:

$$RMSE=\sqrt{\frac{1}{N}\sum_{u,i}(\hat{y}_{u,i} - y_{u,i})^{2}}$$
where N is the number of user/movie combinations, $\hat{y}_{u,i}$ is the predicted rating of user _u_ on movie _i_ and $y_{u,i}$ is the actual prediction of user _u_ on movie _i_. Our aim is to minimize the RMSE during model optimization. 

Alternatives to the RMSE would be the _mean absolute error_ (MAE), which will not be used here.

```{r rmse_function}

RMSE <- function(true_ratings, predicted_ratings) {
  
  sqrt(mean( (true_ratings - predicted_ratings)^2 ))
  
}

```

### Data splitting for modelling

Before we start building our movie recommendation model, we have to split our `edx` dataset again into a train set and a test set, which we will use to generate our model. We will randomly split our dataset using 80% for the train set and 20% for the test set using the `createDataPartition` function of the `caret` package. We also need to filter out users and movies from our test set, which are not part in our train set. Our final model will then be trained on the full `edx` set and will use the `validation` set to evaluate the model's performance.

```{r edx_split_train_test_set}

set.seed(1985, sample.kind = 'Rounding')

#split edx into 80% train and 20% test set
test_index <- createDataPartition(edx$rating, times = 1, p = 0.2, list = FALSE)

train_set <- edx[-test_index,]
test_set <- edx[test_index,]


#remove users + movies from the test_set, which are not in the train_set
test_set <- test_set %>% 
  semi_join(train_set, by = 'movieId') %>% 
  semi_join(train_set, by = 'userId')

```

### Modeling approach


#### Average rating model

The simplest model would be to predict the same rating for all movies independet of users. Such a model would look like this:

$$Y_{u,i} = \mu + \epsilon_{u,i}$$
where $Y_{u,i}$ is the rating by user __u__ of movie __i__, $\mu$ is the _true_ rating for all movies and $\epsilon_{u,i}$ are independent errors sampled from the same distribution centered around 0. The estimate which minimizes the RMSE would be $\mu$. We will use the average of all user ratings $\hat{\mu}$.

Such a model is not expected to be very reliable and needs to be improved using insights gained from the data exploration section.


### Movie Bias Model

We have seen that some movies get rated more often than others. This is intuitive as some movies are more popular than others. We can include such an effect (or _bias_) in our model and see how it improves prediction:

$$Y_{u,i} = \mu + b_{i} + \epsilon_{u,i}$$
where $b_{i}$ is the movie bias.

We will estimate the movie bias $b_{i}$ using the least square estimate $\hat{b}_{i}$, which is the average over all movie ratings minus the estimated mean: $Y_{u,i} - \hat{\mu}$.

In order to calculate our movie prediction, we have to add our estimated movie effect, $\hat{b}_{i}$, to the average rating $\hat{\mu}$.


### User Bias

We will also add another bias term. We observed that user ratings can differ as well - some users are more active than others. We will include a user effect (or _bias_) into our model:

$$Y_{u,i} = \mu + b_{i} + b_{u} + \epsilon_{u,i}$$
where $b_{u}$ is the user bias.


### Time Bias

We saw that the average weekly ratings was time dependent. We can add a time effect to our model, using $d_{u,i}$, which is the day for the rating of _user_ u on _movie_ i. We will use a smoothing function $f(d_{u,i})$, which we will define as $b_{t}$ and average over a full week to include this term to our model:

$$Y_{u,i} = \mu + b_{i} + b_{u} + b_{t} + \epsilon_{u,i}$$


### Regularization

Regularization allows to penalize large estimates from small sample size. One example are movies, which have only one rating. Their ratings without being penalized, would lead to errors and over-fitting of our model.

Here we want to minimize the following equation:

$$\frac{1}{N}\sum_{u,i}=(y_{u,i} - \mu - b_{i} - b_{u} - b_{t} + \lambda (\sum_{i}b_{i}^{2} + \sum_{u}b_{u}^{2} + \sum_{t}b_{t}^{2})$$

The first part of this equation is the least squares estimate, while the second part of the equation penalizes each bias effect.

We can calculate $\hat{b}_{i}$ as 

$$\hat{b}_{i}(\lambda)=\frac{1}{\lambda+n_{i}}\sum_{u=1}^{n_{i}}(Y_{u,i}-\hat{\mu})$$
$\hat{b}_{u}$ as

$$\hat{b}_{u}(\lambda)=\frac{1}{\lambda+n_{i}}\sum_{u=1}^{n_{i}}(Y_{u,i}-\hat{\mu} - \hat{b}_{i})$$

$\hat{b}_{t}$ as

$$\hat{b}_{t}(\lambda)=\frac{1}{\lambda+n_{i}}\sum_{u=1}^{n_{i}}(Y_{u,i}-\hat{\mu} - \hat{b}_{i} - \hat{b}_{u})$$

and we will use cross-validation to pick a $\lambda$, which gives us the smallest RMSE value.



### Matrix factorization

The winning team of the Netflix prize used a model that was exploiting _matrix factorization_. We saw in the data exploration section that not every user rated every movie. We transformed our `edx` data frame into a matrix with user ratings as rows and movies as columns and observed a __sparse matrix__ with missing entries (user ratings) - not every user rated every movie. 

Matrix factorization allows to infer information from high-dimensional data e.g. genres are rated better than other genres, or a certain user preference for certain genres. There are several R packages available e.g. `recommenderlab` or `recosystem`. We will use the `recomendersystem` package here for convenience (vignette of the `recosystem` package^[https://cran.r-project.org/web/packages/recosystem/vignettes/introduction.html]) as this package could be run on a laptop with 8 GB RAM relatively effciently. This package is a wrapper of the `LIBMF` library^[http://www.csie.ntu.edu.tv/~cjlin/libmf], which uses parallel matrix factorization and is open source. The underlying idea is to predict missing values in a rating matrix $R_{m \times n}$ with _m_ users and _n_ movies as an approximation of two matrices of lower dimension $P_{k \times m}$ and $Q_{k \times n}$, such that $$ R \approx \mathbf{P}^\top Q$$. 

Here, we will follow the vignette of the `recosystem` package and use the same values for parameter optimization.


## Results

We will now look at the performance of the different models using first our train and test sets generated from `edx`, and then compare the results side-by-side, using the full `edx` for model training and the `validation` set generated at the very beginning for model validation. Our ultimate aim is to get an RMSE below 0.8649.



### Average only Model

As expected, our simple model has a very weak performance.

```{r average_only_model, echo = TRUE}

mu <- train_set$rating %>% mean()
mu

rmse_df <- tibble(
  Method = 'Average only',
  RMSE = RMSE(test_set$rating, mu)
) 

```

```{r average_model_kable}

rmse_df %>% knitr::kable() %>% kable_styling(latex_options = 'HOLD_position')

```
We will include several effects (_biases_) to our model in the following sections and see if and how an added effect improves the RMSE of our model.



### Movie Bias Model

The first effect to include will be the movie effect:

```{r movie_bias_code, echo=TRUE}


movie_avg <- train_set %>% 
  group_by(movieId) %>% 
  summarize(b_i = mean(rating - mu))


predictions <- test_set %>% 
  left_join(movie_avg, by = 'movieId') %>% 
  mutate(pred = mu + b_i)


rmse_df <-  rmse_df %>% 
  rbind(tibble(
    Method = 'Movie Effect Model',
    RMSE = RMSE(test_set$rating, predictions$pred)
    ))

```

```{r movie_effect_kable}
rmse_df %>% knitr::kable() %>% kable_styling(latex_options = 'HOLD_position')

```
We see that this improves our model by ~10%.



### User Bias Model

The second effect to include will be the user effect:

```{r user_effect_code, echo = TRUE}

user_avg <- train_set %>% 
  left_join(movie_avg, by = 'movieId') %>% 
  group_by(userId) %>% 
  summarize(b_u = mean(rating - mu - b_i))

predictions <- test_set %>% 
  left_join(movie_avg, by = 'movieId') %>% 
  left_join(user_avg, by = 'userId') %>% 
  mutate(pred = mu + b_i + b_u) %>% 
  pull(pred)

rmse_df <- rmse_df %>% 
  rbind(tibble(
    Method = 'Movie + User Effect Model', 
    RMSE = RMSE(test_set$rating, predictions)
    ))

```

```{r user_effect_kable}
rmse_df %>% knitr::kable()

```

As we can see this reduced our RMSE further.


### Time Bias Model

Next we will add a time effect to our model:

```{r time_effect_model_code, echo = TRUE}

weeks_avg <- train_set %>% 
  left_join(movie_avg, by = 'movieId') %>% 
  left_join(user_avg, by = 'userId') %>% 
  group_by(week) %>% 
  summarize(b_t = mean(rating - mu - b_i - b_u))

predictions <- test_set %>% 
  left_join(movie_avg, by = 'movieId') %>% 
  left_join(user_avg, by = 'userId') %>% 
  left_join(weeks_avg, by = 'week') %>% 
  mutate(pred = mu + b_i + b_u + b_t)

rmse_df <- rmse_df %>% 
  rbind(tibble(
    Method = 'Movie + User + Time Effect Model',
    RMSE = RMSE(test_set$rating, predictions$pred)
  ))

```

```{r time_effect_kable}

rmse_df %>% knitr::kable() %>% kable_styling(latex_options = 'HOLD_position')

```
We can see that the improvement our RMSE is only ~0.1%, which is not that great.



### Model regularization

We will now apply regularization to the model with the lowest RMSE, including all effects. We will use a wrapper function, which we can reuse later for the `validation` set. In this wrapper function we will perform cross-validation to find a $\lambda$ which gives the lowest RMSE.

```{r regularization_wrapper_code, echo = TRUE}

lambdas <- seq(0, 10, 0.25)

regularization_wrapper <- function(trainSet, testSet, lambda = lambdas) {
  
  rmses <- sapply(lambda, function(l){
  
  mu <- trainSet$rating %>% mean()
  
  b_i <- trainSet %>% 
    group_by(movieId) %>% 
    summarize(b_i = sum(rating - mu)/(n() + l))
  
  b_u <- trainSet %>% 
    left_join(b_i, by = 'movieId') %>% 
    group_by(userId) %>% 
    summarize(b_u = sum(rating - b_i - mu)/(n() + l))
  
  b_t <- trainSet %>%
    left_join(b_i, by = 'movieId') %>%
    left_join(b_u, by = 'userId') %>%
    group_by(week) %>%
    summarize(b_t = sum(rating - mu - b_i - b_u)/ (n() + l))

  predictions <- testSet %>% 
    left_join(b_i, by = 'movieId') %>% 
    left_join(b_u, by = 'userId') %>% 
    left_join(b_t, by = 'week') %>%
    mutate(pred = mu + b_i + b_u + b_t) %>%
    pull(pred)
  
  return(RMSE(testSet$rating, predictions))
  
  })

}

```
We can apply our wrapper to the train and test sets:
```{r apply_reg_rmses, echo = TRUE}
reg_rmses <- regularization_wrapper(train_set, test_set, lambdas)


rmse_df <- rmse_df %>% 
  rbind(tibble(
    Method = 'Regularization Movie/User/Time effect', 
    RMSE = min(reg_rmses)))

```
and visualize the how different values of lambda perform:

```{r lambda_fig, fig.align='center', fig.width=5, fig.height=3}
qplot(lambdas, reg_rmses, xlab = 'Lambda', ylab = 'RMSE')

```
We see that a $\lambda$ of `r lambdas[which.min(reg_rmses)]` solves our model with the lowest RMSE.

```{r reg_kable}
rmse_df %>% knitr::kable() %>% kable_styling(latex_options = 'HOLD_position')

```
We can see that our RMSE is only taking small steps towards our aim of 0.8649 - can we do better?



### Recosystem Matrix Factorization

We have evolved a model, which got close to our aim of an RMSE < 0.8649. To break through this barrier we will now try a more sophisticated approach using _matrix factorization_. We will use an implementation within the `recosystem` package. As with the regularization model, we will write a wrapper function, which allows to reuse it later with the `validation` set.

```{r recosystem_wrapper, echo = TRUE}


recosystem_wrapper <- function(trainSet, testSet) {
  
  set.seed(1986, sample.kind = "Rounding")
  
  train_data <- with(trainSet, data_memory(user_index = userId,
                                            item_index = movieId,
                                            rating = rating))
  
  
  test_data <- with(testSet, data_memory(user_index = userId,
                                          item_index = movieId,
                                          rating = rating))
  #initialize model object
  r <- Reco()
  
  #parameter optimization
  opts <- r$tune(train_data, 
                 opts = list(dim = c(10,20,30),
                                    lrate = c(0.1, 0.2),
                                    costp_l1 = 0,
                                    costq_l1 = 0,
                                    nthread = 4,
                                    niter = 10, 
                             progress = FALSE))
  
  #train model using optimized parameters
  r$train(train_data, opts = c(opts$min, nthread = 1, niter = 20, verbose=FALSE))
  
  #generate model predictions
  reco_pred <- r$predict(test_data, out_memory())

}

recosystem_train <- recosystem_wrapper(train_set, test_set)

```

```{r rmse_recosystem}

rmse_df <- rmse_df %>% 
  rbind(tibble(
    Method = 'Recosystem Matrix Factorization', 
    RMSE = RMSE(test_set$rating, recosystem_train)))

rmse_df %>% knitr::kable() %>% kable_styling(latex_options = 'HOLD_position')


```
Indeed the matrix factorization based model managed to get an RMSE below 0.8649! Let's move on to model validation and see, how well our model performs with the `validation` set.

### Model validation

We will test the performance of our regularization model and of the Matrix factorization based model, using our wrapper functions and the `validation` set:

```{r model_validation, echo = TRUE}


reg_rmses_val <- regularization_wrapper(edx, validation, lambdas)

recosystem_val <- recosystem_wrapper(edx, validation)

```

```{r rmse_validation_append}

rmse_df <- rmse_df %>% 
  rbind(tibble(
    Method = c('Regularization Model Validation', 'Recosystem Model Validation'), 
    RMSE = c(min(reg_rmses_val), RMSE(validation$rating, recosystem_val))
    ))

rmse_df %>% knitr::kable() %>% kable_styling(latex_options = 'HOLD_position')


```

We see that with the `validation` set, which was data our model has not seen before, we get below 0.8649 with both the regularization model as well as the `recosystem` matrix factorization based model.



## Conclusion

In this project, we have built a machine learning model, which predicts user ratings of movies. We have used two approaches: evolving a linear regression model by adding and penalizing several _bias_ terms and we have explored the `recosystem` package, which uses a matrix factorization based model. We can conclude that just by using a simple linear regression approach, we would have goten close to the _BellKor's Pragmatic Chaos_ winning RMSE of 0.8567^[https://en.wikipedia.org/wiki/Netflix_Prize]. Our model has some limitations and could have been evolved even further. We saw that movie genres are rated different. Furthermore, the data set contains additional information e.g. the release date. One could implement such information and try to further improve the RMSE. Finally our linaer regression model demonstrates that such a lightweight model can be used as a starting point for a machine learning (_"why bother training complicated models, if they are not better than linear regression?"_). 

Using the `recosystem` package we achieved an RMSE well below our aim of 0.8649. The package showed fast performance on a slim laptop and was easy to try out. It would be interesting to test this package on other data sets in the future and see its performance. Using such a sophisticated model, demonstrates another common issue in machine learning: model interpretability. Although we achieved better performance (lower RMSE), we increased the black-box character as compared to the linear regression model.





